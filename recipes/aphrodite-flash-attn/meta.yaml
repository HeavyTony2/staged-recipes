{% set name = "aphrodite-flash-attn" %}
{% set version = "2.6.1.post2" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://github.com/AlpinDale/flash-attention/archive/v{{ version }}.tar.gz
  sha256: 081ff239ec3008c2b9d298795fa76de57025beabbdf9dd092f9ffa7f463ae318

build:
  skip: true  # [(linux and cuda_compiler_version != "12.4") and not linux]
  script: {{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation
  number: 0

requirements:
  build:
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}
    - {{ stdlib("c") }}
  host:
    - python
    - psutil
    - pip
    - pytorch ==2.4.0
    - setuptools
    - cuda-version {{ cuda_compiler_version }}
    - cutlass
  run:
    - python
    - pytorch ==2.4.0
    - cuda-version {{ cuda_compiler_version }}

test:
  imports:
    - aphrodite_flash_attn
  commands:
    - pip check
  requires:
    - pip

about:
  home: https://github.com/AlpinDale/flash-attention
  summary: Cross-platform lib for process and system monitoring in Python.
  license: BSD-3-Clause
  license_file: LICENSE
  dev_url: https://github.com/AlpinDale/flash-attention

extra:
  recipe-maintainers:
    - HeavyTony2
